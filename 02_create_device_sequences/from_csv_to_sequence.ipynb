{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipaddress\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_from_csv(date, source_folder, destination_folder):\n",
    "    \"\"\"\n",
    "    Turn raw packet data into one sequence per device. Store the result in CSV format. The result\n",
    "    has all preprocessed features we need for LSTM training.\n",
    "    \n",
    "    Arguments:\n",
    "        date: Name of the CSV file with packet data (without file extension). The results are also stored in a folder with the date name.\n",
    "        source_folder: path to folder with raw CSV data\n",
    "        destination_folder: path to folder where the result files will be written to\n",
    "    \"\"\"\n",
    "\n",
    "    # skip if destination folder already exists\n",
    "    path = \"{}/{}\".format(destination_folder, date)\n",
    "    if os.path.exists(path):\n",
    "        print(\"Folder {} already exists. Skip processing.\".format(path))\n",
    "        return\n",
    "    else:\n",
    "        print(\"Start processing data for {}.\".format(path))\n",
    "        os.makedirs(path)\n",
    "\n",
    "    macs = [\"d0:52:a8:00:67:5e\", \"44:65:0d:56:cc:d3\", \"70:ee:50:18:34:43\",\n",
    "      \"f4:f2:6d:93:51:f1\", \"00:16:6c:ab:6b:88\", \"30:8c:fb:2f:e4:b2\",\n",
    "      \"00:62:6e:51:27:2e\", \"e8:ab:fa:19:de:4f\", \"00:24:e4:11:18:a8\",\n",
    "      \"ec:1a:59:79:f4:89\", \"50:c7:bf:00:56:39\", \"74:c6:3b:29:d7:1d\",\n",
    "      \"ec:1a:59:83:28:11\", \"18:b4:30:25:be:e4\", \"70:ee:50:03:b8:ac\",\n",
    "      \"00:24:e4:1b:6f:96\", \"74:6a:89:00:2e:25\", \"00:24:e4:20:28:c6\",\n",
    "      \"d0:73:d5:01:83:08\", \"18:b7:9e:02:20:44\", \"e0:76:d0:33:bb:85\",\n",
    "      \"70:5a:0f:e4:9b:c0\", \"08:21:ef:3b:fc:e3\", \"30:8c:fb:b6:ea:45\",\n",
    "      \"40:f3:08:ff:1e:da\", \"74:2f:68:81:69:42\", \"ac:bc:32:d4:6f:2f\",\n",
    "      \"b4:ce:f6:a7:a3:c2\", \"d0:a6:37:df:a1:e1\", \"f4:5c:89:93:cc:85\",\n",
    "      \"14:cc:20:51:33:ea\"]\n",
    "\n",
    "    for device in macs:\n",
    "        # import csv to pandas dataframe\n",
    "        file = \"{}/{}.csv\".format(source_folder, date)\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        # filter by one device\n",
    "        df=df.query('MAC_source.str.contains(\"{}\") | MAC_destination.str.contains(\"{}\")'.format(device, device))\n",
    "\n",
    "        # sort by time stamp\n",
    "        # note: some packets in the PCAP data are out of order, thus this sorting step is important\n",
    "        df = df.sort_values('TIME')\n",
    "\n",
    "        # add Outbound: +1: outbound, -1: inbound\n",
    "        direction = [1 if x == device else -1 for x in df['MAC_source']]\n",
    "        df['Outbound'] = direction\n",
    "\n",
    "        # Time since last packet\n",
    "        times = list(df['TIME'])\n",
    "        first_timestamp = times[0] if len(times) > 0 else 0\n",
    "        times.insert(0, first_timestamp)\n",
    "        times.pop()\n",
    "        np_times = np.array(times)\n",
    "        np_original = np.array(list(df['TIME']))\n",
    "        np_intervals = np_original - np_times\n",
    "        df['Time_since_last_packet'] = np_intervals.tolist()\n",
    "\n",
    "        # compute field IP_source_internal and IP_destination_internal.\n",
    "        # internal: +1\n",
    "        # external: -1\n",
    "        internal_source = [ipaddress.ip_address(ip).is_private for ip in df['IP_source']]\n",
    "        df['IP_source_internal'] = [1 if internal else -1 for internal in internal_source]\n",
    "        internal_destination = [ipaddress.ip_address(ip).is_private for ip in df['IP_destination']]\n",
    "        df['IP_destination_internal'] = [1 if internal else -1 for internal in internal_destination]\n",
    "\n",
    "        # compute column \"Port_class_source\" and \"Port_class_destination\"\n",
    "        df['Port_class_source'] = [port if (0 < int(port) < 1024) else -1 for port in df['Port_source']]\n",
    "        df['Port_class_destination'] = [port if (0 < int(port) < 1024) else -1 for port in df['Port_destination']]\n",
    "\n",
    "        # keep only data for training\n",
    "        df=df.drop(['TIME', 'MAC_source', 'MAC_destination'], axis=1)\n",
    "\n",
    "        # verify there is non NaN, infinity or negative data for \"Time_since_last_packet\"\n",
    "        time_intervals = df['Time_since_last_packet'].to_numpy()\n",
    "        finite = np.isfinite(time_intervals)\n",
    "        if (finite == False).sum() > 0:\n",
    "            sys.exit('There are infinite time intervals in csv {}'.format(file))\n",
    "        if np.isnan(time_intervals).any():\n",
    "            sys.exit('There are NaN time intervals in csv {}'.format(file))\n",
    "        if (time_intervals < 0).any():\n",
    "            sys.exit('There are negative time intervals in csv {}'.format(file))\n",
    "\n",
    "        # export data for training\n",
    "        device_mac_for_file_name = device.replace(':', '-')\n",
    "        df.to_csv(\"{}/{}/{}.csv\".format(destination_folder, date, device_mac_for_file_name), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing data for ../../csv_sequences/16-10-28.\n"
     ]
    }
   ],
   "source": [
    "# create csv sequence for a few dates\n",
    "\n",
    "dates = [\n",
    "    \"16-10-28\"\n",
    "]\n",
    "\n",
    "for date in dates:\n",
    "    create_features_from_csv(date, \"../../csv_from_pcap\", \"../../csv_sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing data for ../../csv_sequences/16-09-23.\n",
      "Start processing data for ../../csv_sequences/16-10-04.\n",
      "Start processing data for ../../csv_sequences/16-10-15.\n",
      "Start processing data for ../../csv_sequences/16-10-26.\n",
      "Start processing data for ../../csv_sequences/16-11-07.\n",
      "Start processing data for ../../csv_sequences/16-11-18.\n",
      "Start processing data for ../../csv_sequences/16-09-24.\n",
      "Start processing data for ../../csv_sequences/16-10-05.\n",
      "Start processing data for ../../csv_sequences/16-10-16.\n",
      "Start processing data for ../../csv_sequences/16-10-27.\n",
      "Start processing data for ../../csv_sequences/16-11-08.\n",
      "Start processing data for ../../csv_sequences/16-11-19.\n",
      "Start processing data for ../../csv_sequences/16-09-25.\n",
      "Start processing data for ../../csv_sequences/16-10-06.\n",
      "Start processing data for ../../csv_sequences/16-10-17.\n",
      "Start processing data for ../../csv_sequences/16-10-28.\n",
      "Start processing data for ../../csv_sequences/16-11-09.\n",
      "Start processing data for ../../csv_sequences/16-11-20.\n",
      "Start processing data for ../../csv_sequences/16-09-26.\n",
      "Start processing data for ../../csv_sequences/16-10-07.\n",
      "Start processing data for ../../csv_sequences/16-10-18.\n",
      "Start processing data for ../../csv_sequences/16-10-29.\n",
      "Start processing data for ../../csv_sequences/16-11-10.\n",
      "Start processing data for ../../csv_sequences/16-11-21.\n",
      "Start processing data for ../../csv_sequences/16-09-27.\n",
      "Start processing data for ../../csv_sequences/16-10-08.\n",
      "Start processing data for ../../csv_sequences/16-10-19.\n",
      "Start processing data for ../../csv_sequences/16-10-30.\n",
      "Start processing data for ../../csv_sequences/16-11-11.\n",
      "Start processing data for ../../csv_sequences/16-11-22.\n",
      "Start processing data for ../../csv_sequences/16-09-28.\n",
      "Start processing data for ../../csv_sequences/16-10-09.\n",
      "Start processing data for ../../csv_sequences/16-10-20.\n",
      "Start processing data for ../../csv_sequences/16-10-31.\n",
      "Start processing data for ../../csv_sequences/16-11-12.\n",
      "Start processing data for ../../csv_sequences/16-09-29.\n",
      "Start processing data for ../../csv_sequences/16-10-10.\n",
      "Start processing data for ../../csv_sequences/16-10-21.\n",
      "Start processing data for ../../csv_sequences/16-11-01.\n",
      "Start processing data for ../../csv_sequences/16-11-13.\n",
      "Start processing data for ../../csv_sequences/16-09-30.\n",
      "Start processing data for ../../csv_sequences/16-10-11.\n",
      "Start processing data for ../../csv_sequences/16-10-22.\n",
      "Start processing data for ../../csv_sequences/16-11-02.\n",
      "Start processing data for ../../csv_sequences/16-11-14.\n",
      "Folder ../../csv_sequences/16-10-01 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-10-12 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-10-23 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-11-04 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-11-15 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-10-02 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-10-13 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-10-24 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-11-05 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-11-16 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-10-03 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-10-14 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-10-25 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-11-06 already exists. Skip processing.\n",
      "Folder ../../csv_sequences/16-11-17 already exists. Skip processing.\n"
     ]
    }
   ],
   "source": [
    "# create csv sequencees for all dates\n",
    "\n",
    "dates = [\n",
    "    \"16-09-23\", \"16-10-04\", \"16-10-15\", \"16-10-26\", \"16-11-07\", \"16-11-18\",\n",
    "    \"16-09-24\", \"16-10-05\", \"16-10-16\", \"16-10-27\", \"16-11-08\", \"16-11-19\",\n",
    "    \"16-09-25\", \"16-10-06\", \"16-10-17\", \"16-10-28\", \"16-11-09\", \"16-11-20\",\n",
    "    \"16-09-26\", \"16-10-07\", \"16-10-18\", \"16-10-29\", \"16-11-10\", \"16-11-21\",\n",
    "    \"16-09-27\", \"16-10-08\", \"16-10-19\", \"16-10-30\", \"16-11-11\", \"16-11-22\",\n",
    "    \"16-09-28\", \"16-10-09\", \"16-10-20\", \"16-10-31\", \"16-11-12\",\n",
    "    \"16-09-29\", \"16-10-10\", \"16-10-21\", \"16-11-01\", \"16-11-13\",\n",
    "    \"16-09-30\", \"16-10-11\", \"16-10-22\", \"16-11-02\", \"16-11-14\",\n",
    "    \"16-10-01\", \"16-10-12\", \"16-10-23\", \"16-11-04\", \"16-11-15\",\n",
    "    \"16-10-02\", \"16-10-13\", \"16-10-24\", \"16-11-05\", \"16-11-16\",\n",
    "    \"16-10-03\", \"16-10-14\", \"16-10-25\", \"16-11-06\", \"16-11-17\"\n",
    "]\n",
    "\n",
    "for date in dates:\n",
    "    create_features_from_csv(date, \"../../csv_from_pcap\", \"../../csv_sequences\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc762cda6b2e5889b0d7e4f51ac3925eadcbd72dc53dbc27591afad75b09441e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
