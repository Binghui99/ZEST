{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9dxgm12XDN4",
        "outputId": "b958f49c-9e82-4d75-f01e-bd3035b5dad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout, Reshape\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Input\n",
        "from collections import Counter\n",
        "from numpy import savez_compressed\n",
        "from datetime import date\n",
        "from numpy import load\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# google colab file paths\n",
        "CSV_SEQUENCES = \"/content/gdrive/MyDrive/Colab Notebooks/iot_device_classification/new_csv_sequences\"\n",
        "NPZ_WINDOWS = \"/content/gdrive/MyDrive/Colab Notebooks/iot_device_classification/npz_windows\"\n",
        "MODELS = \"/content/gdrive/MyDrive/Colab Notebooks/iot_device_classification/models\"\n",
        "\n",
        "\n",
        "\n",
        "# for Google colab: check for gpu\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "# else:\n",
        "#   print(gpu_info)\n",
        "\n",
        "\n",
        "\n",
        "# for Google colab: check RAM\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5w6v1hNWVat",
        "outputId": "c714368c-119d-4c0f-c16b-2cd1728058af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train windws: (1071459, 200, 8)\n",
            "shape of train labels: (1071459, 1, 28)\n",
            "shape of test windows: (267865, 200, 8)\n",
            "shape of test labels: (267865, 1, 28)\n"
          ]
        }
      ],
      "source": [
        "def load_data_make_split(npz_file, train_percentage):\n",
        "    \"\"\"\n",
        "    Load training data (windows + one-hot labels) from compressed file. Split data into train and test set\n",
        "\n",
        "    Arguments:\n",
        "        - npz_file: The path to the *.npz file\n",
        "        - train_percentage: the percentage of data used for training (and not testing), e.g. 0.8\n",
        "    Returns:\n",
        "        A 4-tuple of train and test data with labels: (x_train, y_train, x_test, y_test)\n",
        "    \"\"\"\n",
        "    dict_data = load(npz_file)\n",
        "    x = dict_data['x']\n",
        "    y = dict_data['y']\n",
        "    train_length = int(len(x)*train_percentage)\n",
        "    x_train = x[:train_length]\n",
        "    y_train = y[:train_length]\n",
        "    x_test = x[train_length:]\n",
        "    y_test = y[train_length:]\n",
        "    return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "# test load_data_make_split()\n",
        "x_train, y_train, x_test, y_test = load_data_make_split(\"{}/update_new_feature_all_days_all_devices.npz\".format(NPZ_WINDOWS), 0.8)\n",
        "print(\"shape of train windws: {}\".format(x_train.shape))\n",
        "print(\"shape of train labels: {}\".format(y_train.shape))\n",
        "print(\"shape of test windows: {}\".format(x_test.shape))\n",
        "print(\"shape of test labels: {}\".format(y_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d2cUv0jzQu3",
        "outputId": "e95aa9b2-ff7d-4677-cd8e-9746d2a7e1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(24, 12041), (15, 12270), (6, 14034), (14, 15738), (2, 34720), (0, 36454), (27, 55804), (7, 61729), (1, 67218), (4, 68597), (10, 102906), (19, 111726), (22, 197876), (23, 228604), (5, 275855)]\n"
          ]
        }
      ],
      "source": [
        "all_labels = []\n",
        "for i in range(len(y_train)):\n",
        "  index = np.where(y_train[i][0] == True)\n",
        "  k = index[0][0]\n",
        "  all_labels.append(k)\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "  index = np.where(y_test[i][0] == True)\n",
        "  k = index[0][0]\n",
        "  all_labels.append(k)\n",
        "\n",
        "unique, count = np.unique(all_labels, return_counts = True)\n",
        "useable_data = []\n",
        "data_pair =dict()\n",
        "for i in range(len(unique)):\n",
        "  if count[i] > 10000:\n",
        "    useable_data.append(unique[i])\n",
        "    # print(unique[i], count[i])\n",
        "    data_pair[unique[i]] = count[i]\n",
        "sorted_data_by_counts = sorted(data_pair.items(), key=lambda x:x[1])\n",
        "print(sorted_data_by_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljKJ4274hA3U"
      },
      "outputs": [],
      "source": [
        "all_device_names_dict = {0:'Smart Things', 1: 'Amazon Echo', 2:'Netatmo Welcome',3:'TP-Link Day Night Cloud camera', 4:'Samsung SmartCam', 5: 'Dropcam', 6: 'Withings Smart Baby Monitor', 7:'Belkin Wemo switch', 8:'TP-Link Smart plug',\n",
        "                         9: 'iHome', 10:'Belkin wemo motion sensor', 11:'NEST Protect smoke alarm', 12:'Netatmo weather station',13:'Withings Smart scale',14:'Withings Aura smart sleep sensor',15:'Light Bulbs LiFX Smart Bulb',\n",
        "                         16: 'Triby Speaker', 17:'PIX-STAR Photo-frame', 18 : 'HP Printer', 19: 'Samsung Galaxy Tab', 20: 'Nest Dropcam', 21:'Android Phone', 22:'Laptop', 23:'MacBook', 24:'Android Phone',\n",
        "                         25: 'IPhone', 26:'MacBook/Iphone', 27:'Insteon Camera'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYa5bk7ZggaW",
        "outputId": "524f1d77-8194-4aa0-bc53-ac8d680699e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 5, 6, 10, 14, 15, 22, 23, 24]\n",
            "[7, 2]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "total_av_idx = [0, 1, 2, 5, 6, 7, 10, 14, 15, 22, 23, 24]\n",
        "unseen_idx = random.sample(total_av_idx, 2)\n",
        "# unseen_idx = [23,2]\n",
        "seen_idx = [i for i in total_av_idx if i not in unseen_idx]\n",
        "print(seen_idx)\n",
        "print(unseen_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAbrbpzazoDD"
      },
      "outputs": [],
      "source": [
        "num_unseen = len(unseen_idx)\n",
        "num_seen = len(seen_idx)\n",
        "\n",
        "\n",
        "# def one-hot index\n",
        "def idxtoOneHot(idx, length):\n",
        "  label_ohe = np.zeros((1,length))\n",
        "  label_ohe[0][idx] = 1\n",
        "  return label_ohe\n",
        "\n",
        "# def merge\n",
        "def Merge(dict1, dict2):\n",
        "  res = {**dict1, **dict2}\n",
        "  return res\n",
        "\n",
        "\n",
        "# def dic\n",
        "def generate_label_dict(unseen_idx, seed_idx):\n",
        "  unseen_dict = dict()\n",
        "  seen_dict = dict()\n",
        "  all_index_to_name = dict()\n",
        "  seen_index_to_name = dict()\n",
        "  unseen_index_to_name = dict()\n",
        "  for i in range(0,num_seen):\n",
        "    seen_dict[seen_idx[i]] = i\n",
        "    seen_index_to_name[i] = all_device_names_dict[seen_idx[i]]\n",
        "  for i in range(0,2):\n",
        "    unseen_dict[unseen_idx[i]] = num_seen + i\n",
        "    unseen_index_to_name[num_seen+i] = all_device_names_dict[unseen_idx[i]]\n",
        "  total_dict = Merge(unseen_dict, seen_dict)\n",
        "  all_index_to_name = Merge(seen_index_to_name, unseen_index_to_name)\n",
        "  return unseen_dict, seen_dict, total_dict,all_index_to_name\n",
        "\n",
        "\n",
        "def seen_training_data_for_LSTM(init_x_train, init_x_test, init_y_train, init_y_test, seen_dict):\n",
        "  x_train_feature = []\n",
        "  y_train_feature = []\n",
        "  x_test_feature = []\n",
        "  y_test_feature = []\n",
        "\n",
        "  for i in range(len(init_y_train)):\n",
        "    index = np.where(init_y_train[i][0] == True)\n",
        "    k = index[0][0]\n",
        "    if k in seen_idx:\n",
        "      x_train_feature.append(init_x_train[i])\n",
        "      # idx to new range\n",
        "      new_k = seen_dict[k]\n",
        "      y_train_feature.append(idxtoOneHot(new_k,len(seen_idx)))\n",
        "\n",
        "  for i in range(len(init_y_test)):\n",
        "    index = np.where(init_y_test[i][0] == True)\n",
        "    k = index[0][0]\n",
        "    if k in seen_idx:\n",
        "      x_test_feature.append(init_x_test[i])\n",
        "      # idx to new range\n",
        "      new_k = seen_dict[k]\n",
        "      y_test_feature.append(idxtoOneHot(new_k,len(seen_idx)))\n",
        "  return np.array(x_train_feature), np.array(y_train_feature), np.array(x_test_feature), np.array(y_test_feature)\n",
        "\n",
        "def feature_extraction_data_seen_and_unseen(init_x_train, init_x_test, init_y_train, init_y_test, attr_dict):\n",
        "\n",
        "  x_train_attr = []\n",
        "  y_train_attr = []\n",
        "  x_test_attr = []\n",
        "  y_test_attr = []\n",
        "\n",
        "  attr_idx = total_av_idx\n",
        "  for i in range(len(init_y_train)):\n",
        "    index = np.where(init_y_train[i][0] == True)\n",
        "    k = index[0][0]\n",
        "    if k in attr_idx:\n",
        "      x_train_attr.append(init_x_train[i])\n",
        "      # idx to new range\n",
        "      new_k = attr_dict[k]\n",
        "      y_train_attr.append(idxtoOneHot(new_k,len(attr_idx)))\n",
        "\n",
        "  for i in range(len(init_y_test)):\n",
        "    index = np.where(init_y_test[i][0] == True)\n",
        "    k = index[0][0]\n",
        "    if k in attr_idx:\n",
        "      x_test_attr.append(init_x_test[i])\n",
        "      # idx to new range\n",
        "      new_k = attr_dict[k]\n",
        "      y_test_attr.append(idxtoOneHot(new_k,len(attr_idx)))\n",
        "\n",
        "  x_train_attr = np.array(x_train_attr)\n",
        "  y_train_attr = np.array(y_train_attr)\n",
        "  x_test_attr = np.array(x_test_attr)\n",
        "  y_test_attr = np.array(y_test_attr)\n",
        "\n",
        "  return x_train_attr,  y_train_attr, x_test_attr, y_test_attr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDc2DMvr0Axe"
      },
      "outputs": [],
      "source": [
        "unseen_dict, seen_dict,total_dict,all_index_to_name = generate_label_dict(unseen_idx, seen_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOs2md-v0A5L"
      },
      "outputs": [],
      "source": [
        "x_train_feature, y_train_feature, x_test_feature, y_test_feature = seen_training_data_for_LSTM(x_train, x_test, y_train, y_test, seen_dict)\n",
        "x_train_attr,  y_train_attr, x_test_attr, y_test_attr = feature_extraction_data_seen_and_unseen(x_train, x_test, y_train, y_test, total_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTh2GwVI0A7P"
      },
      "outputs": [],
      "source": [
        "print('unseen dictionary', unseen_dict)\n",
        "print('seen dictionary', seen_dict)\n",
        "print('total dictionary', total_dict)\n",
        "print('index to name dictionary', all_index_to_name)\n",
        "\n",
        "print(\"shape of train windws: {}\".format(x_train_attr.shape))\n",
        "print(\"shape of train labels: {}\".format(y_train_attr.shape))\n",
        "print(\"shape of test windows: {}\".format(x_test_attr.shape))\n",
        "print(\"shape of test labels: {}\".format(y_test_attr.shape))\n",
        "\n",
        "print(\"shape of train windws: {}\".format(x_train_feature.shape))\n",
        "print(\"shape of train labels: {}\".format(y_train_feature.shape))\n",
        "print(\"shape of test windows: {}\".format(x_test_feature.shape))\n",
        "print(\"shape of test labels: {}\".format(y_test_feature.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnYI8KEOYXlg"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "CLASSES=10\n",
        "PATIENCE= 10\n",
        "EPOCHS= 30\n",
        "BATCH_SIZE = 64\n",
        "DROPOUT = 0.2\n",
        "LEARNING_RATE=0.0001\n",
        "unseen_number = 12 - CLASSES\n",
        "today = date.today()\n",
        "\n",
        "\n",
        "model_feature = tf.keras.models.Sequential([\n",
        "    Input(shape=x_train_feature[0].shape, dtype = tf.float32),\n",
        "    # Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(1e-4))),\n",
        "    Bidirectional(LSTM(64, return_sequences=False, kernel_regularizer=l2())),\n",
        "    # Dense(units=32, activation = 'relu', kernel_regularizer=l2()),\n",
        "    Dense(units=20, activation = 'relu', kernel_regularizer=l2(), name = 'My_Feature'),\n",
        "    Dropout(DROPOUT),\n",
        "    Dense(units=3, activation = 'relu', kernel_regularizer=l2(), name = 'My_Atrribute'),\n",
        "    Dense(units=CLASSES, activation = 'softmax', kernel_regularizer=l2()),\n",
        "    Reshape([1, -1]),\n",
        "])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, mode='min')\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"{}/deft_LSTM_Kmeans_RF_10-2_test\".format(MODELS), monitor='val_loss', verbose=0,\n",
        "                                    save_best_only=True, mode='min')\n",
        "\n",
        "model_feature.compile(loss='categorical_crossentropy',\n",
        "                   optimizer=Adam(learning_rate = LEARNING_RATE),\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# do training without unseen classes\n",
        "history = model_feature.fit(x_train_feature, y_train_feature, epochs=EPOCHS,\n",
        "                    validation_data=(x_test_feature,y_test_feature),\n",
        "                    callbacks=[early_stopping, checkpoint],\n",
        "                    batch_size = BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL5p6KlYUC4E"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOFUQt_gUMPh"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model acc')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6sdjf2kOZw7"
      },
      "outputs": [],
      "source": [
        "CLASSES=10\n",
        "PATIENCE= 5\n",
        "EPOCHS= 20\n",
        "BATCH_SIZE = 64\n",
        "DROPOUT = 0.2\n",
        "LEARNING_RATE=0.001\n",
        "unseen_number = 12 - CLASSES\n",
        "\n",
        "model_feature = tf.keras.models.load_model(\"{}/deft_LSTM_Kmeans_RF_10-2_test\".format(MODELS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo9ky41ZYXtH",
        "outputId": "a5d97081-40dc-46da-c811-42356fb2247d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1059445, 200, 8)\n",
            "(1059445, 1, 12)\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# take the layer out\n",
        "latent_space_model = tf.keras.Model(inputs = model_feature.input, outputs = model_feature.get_layer('My_Feature').output)\n",
        "attr_model = tf.keras.Model(inputs = model_feature.input, outputs = model_feature.get_layer('My_Atrribute').output)\n",
        "\n",
        "new_x = np.concatenate((x_train_attr,x_test_attr))\n",
        "new_y = np.concatenate((y_train_attr,y_test_attr))\n",
        "print(new_x.shape)\n",
        "print(new_y.shape)\n",
        "\n",
        "\n",
        "embedding_set = []\n",
        "attr_set = []\n",
        "for i in range(0,new_x.shape[0],1024):\n",
        "  if i % 100000 == 0:\n",
        "    print(i)\n",
        "  if i+1024 <= new_x.shape[0]:\n",
        "    embedding_set = [*embedding_set, *latent_space_model(new_x[i:i+1024,:,:])]\n",
        "    attr_set = [*attr_set, *attr_model(new_x[i:i+1024,:,:])]\n",
        "  else:\n",
        "    embedding_set = [*embedding_set, *latent_space_model(new_x[i:,:,:])]\n",
        "    attr_set = [*attr_set, *attr_model(new_x[i:,:,:])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUptruFiXEo_"
      },
      "outputs": [],
      "source": [
        "def new_attr(mean, maxi, mini):\n",
        "  new = []\n",
        "  for i in range(len(mean)):\n",
        "    x_scale = 1 * ((mean[i] - mini) / (maxi-mini))\n",
        "    new.append(np.round(x_scale,2))\n",
        "  return new\n",
        "\n",
        "def generate_attr_dict(new_y):\n",
        "  attr_dict = dict()\n",
        "  for i in range(12):\n",
        "    attr_dict[i] = []\n",
        "\n",
        "  for i in range(len(new_y)):\n",
        "    index = np.where(new_y[i][0] == True)\n",
        "    k = index[0][0]\n",
        "    attr_dict[k].append(attr_set[i])\n",
        "\n",
        "  attr_lib = dict()\n",
        "  for key in attr_dict:\n",
        "    print(key)\n",
        "    attr_lib[key] = np.mean(attr_dict[key],axis = 0)\n",
        "\n",
        "\n",
        "  max_for_all = []\n",
        "  min_for_all = []\n",
        "  for i in attr_lib.keys():\n",
        "    k = np.max(attr_lib[i])\n",
        "    max_for_all.append(k)\n",
        "    l = np.min(attr_lib[i])\n",
        "    min_for_all.append(l)\n",
        "\n",
        "\n",
        "  maxi = np.max(max_for_all)\n",
        "  mini = np.min(min_for_all)\n",
        "\n",
        "  # print(maxi)\n",
        "  # print(mini)\n",
        "\n",
        "  new_attr_lib = dict()\n",
        "  for key in attr_lib:\n",
        "    new_attr_lib[key] = new_attr(attr_lib[key],maxi, mini)\n",
        "\n",
        "  new_attr_set = []\n",
        "  new_y_label = []\n",
        "  for i in range(len(new_y)):\n",
        "      index = np.where(new_y[i][0] == True)\n",
        "      k = index[0][0]\n",
        "      new_y_label.append(k)\n",
        "      new_attr_set.append(new_attr_lib[k])\n",
        "\n",
        "  return new_attr_lib,new_y_label,new_attr_set, attr_lib\n",
        "\n",
        "\n",
        "new_attr_lib, new_y_label, new_attr_set, attr_lib = generate_attr_dict(new_y)\n",
        "\n",
        "\n",
        "print(new_attr_lib)\n",
        "print(attr_lib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phCyb7bfQALP",
        "outputId": "a371d6dc-c57c-420f-ce1d-0f92cd5a0462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: array([13.881974, 14.364426, 10.775362], dtype=float32), 1: array([19.818201 ,  1.6539674,  0.6376107], dtype=float32), 2: array([17.585768  ,  0.13264282, 14.816468  ], dtype=float32), 3: array([4.625749  , 0.09583263, 0.6082011 ], dtype=float32), 4: array([ 0.62221414,  6.69359   , 10.333091  ], dtype=float32), 5: array([ 0.9619117 ,  0.62450963, 11.146729  ], dtype=float32), 6: array([4.803446, 4.32539 , 4.328219], dtype=float32), 7: array([0.15496694, 3.4552805 , 0.09663309], dtype=float32), 8: array([ 4.9870687 , 18.414371  ,  0.24464454], dtype=float32), 9: array([12.740985  , 13.265031  ,  0.15779985], dtype=float32), 10: array([9.70206  , 8.3818865, 1.2241544], dtype=float32), 11: array([7.8620157, 8.399073 , 1.7808505], dtype=float32)}\n",
            "Counter({2: 275855, 8: 197876, 5: 102906, 1: 67218, 4: 61729, 0: 36454, 6: 15738, 3: 14034, 7: 12270, 9: 12041})\n",
            "Counter({10: 228604, 11: 34720})\n",
            "10 2 3\n"
          ]
        }
      ],
      "source": [
        "seen_index = [0,1,2,3,4,5,6,7,8,9]\n",
        "unseen_index = [10,11]\n",
        "print(attr_lib)\n",
        "seen_num = len(seen_index)\n",
        "unseen_num = len(unseen_index)\n",
        "attr_num = 3\n",
        "\n",
        "\n",
        "def generate_npz_for_seen_and_unseen(seen_idx, unseen_idx, new_y_label, new_attr_set,embedding_set, NPZ_WINDOWS):\n",
        "  seen_class_label = []\n",
        "  seen_class_attr = []\n",
        "  seen_class_features = []\n",
        "\n",
        "  unseen_class_label = []\n",
        "  unseen_class_attr  = []\n",
        "  unseen_class_features  = []\n",
        "\n",
        "  for i in range(len(new_y_label)):\n",
        "    if new_y_label[i] in seen_index:\n",
        "      seen_class_label.append(new_y_label[i])\n",
        "      seen_class_attr.append(new_attr_set[i])\n",
        "      seen_class_features.append(embedding_set[i])\n",
        "    elif new_y_label[i] in unseen_index:\n",
        "      unseen_class_label.append(new_y_label[i])\n",
        "      unseen_class_attr.append(new_attr_set[i])\n",
        "      unseen_class_features.append(embedding_set[i])\n",
        "\n",
        "  print(Counter(seen_class_label))\n",
        "  print(Counter(unseen_class_label))\n",
        "\n",
        "  today = date.today()\n",
        "  seen_num = len(seen_idx)\n",
        "  unseen_num = len(unseen_idx)\n",
        "  attr_num = len(seen_class_attr[0])\n",
        "  print(seen_num, unseen_num, attr_num)\n",
        "\n",
        "\n",
        "  npz_file_seen = \"{}/Deft_biLSTM_rf_seen_data\".format(NPZ_WINDOWS)\n",
        "  savez_compressed(npz_file_seen, x = seen_class_features, y = seen_class_label, attribute = seen_class_attr)\n",
        "\n",
        "  npz_file_unseen = \"{}/Deft_biLSTM_rf_unseen_data\".format(NPZ_WINDOWS)\n",
        "  savez_compressed(npz_file_unseen, x = unseen_class_features, y = unseen_class_label, attribute = unseen_class_attr)\n",
        "\n",
        "\n",
        "  return seen_class_features,seen_class_label, seen_class_attr, unseen_class_features, unseen_class_label, unseen_class_attr, npz_file_seen, npz_file_unseen\n",
        "\n",
        "\n",
        "seen_class_features,seen_class_label, seen_class_attr, unseen_class_features, unseen_class_label, unseen_class_attr, npz_file_seen, npz_file_unseen = generate_npz_for_seen_and_unseen(seen_index, unseen_index, new_y_label, new_attr_set,embedding_set,NPZ_WINDOWS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GoG6MlZlT1U"
      },
      "outputs": [],
      "source": [
        "# KNN ideas\n",
        "new_y_train = []\n",
        "for i in range(len(new_y)):\n",
        "  index = np.where(new_y[i][0] == True)\n",
        "  k = index[0][0]\n",
        "  new_y_train.append(k)\n",
        "\n",
        "\n",
        "kmean_z_train = []\n",
        "kmean_y_train = []\n",
        "\n",
        "data_dict = dict()\n",
        "for i in range(12):\n",
        "  data_dict[i] = []\n",
        "\n",
        "for i in range(len(new_y_train)):\n",
        "  m = data_dict[new_y_train[i]]\n",
        "  if len(m) <= 10000:\n",
        "    m.append(attr_set[i])\n",
        "    data_dict[new_y_train[i]] = m\n",
        "\n",
        "for i in range(12):\n",
        "  for j in range(10000):\n",
        "    kmean_z_train.append(data_dict[i][j])\n",
        "    kmean_y_train.append(i)\n",
        "\n",
        "\n",
        "seen_x = []\n",
        "seen_y = []\n",
        "for i in range(10):\n",
        "  for j in range(2000):\n",
        "    seen_x.append(data_dict[i][j])\n",
        "    seen_y.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_30z6aN0THYn"
      },
      "outputs": [],
      "source": [
        "seen_x = np.array(seen_x)\n",
        "seen_y = np.array(seen_y)\n",
        "print(seen_x.shape)\n",
        "print(seen_y.shape)\n",
        "\n",
        "print(np.unique(seen_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlv0ih-TlT7x",
        "outputId": "6acd5f30-947d-403c-a2d5-b257e2a1df55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(120000, 3)\n",
            "(120000,)\n"
          ]
        }
      ],
      "source": [
        "kmean_z_train = np.array(kmean_z_train)\n",
        "kmean_y_train = np.array(kmean_y_train)\n",
        "print(kmean_z_train.shape)\n",
        "print(kmean_y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds4iihy8eorV"
      },
      "outputs": [],
      "source": [
        "def data_split(x, y, ratio1, ratio2):\n",
        "\n",
        "  rng_state = np.random.get_state()\n",
        "  np.random.shuffle(x)\n",
        "  np.random.set_state(rng_state)\n",
        "  np.random.shuffle(y)\n",
        "\n",
        "  train_length = int(len(x)*ratio1)\n",
        "  valid_length = int(len(x)*(ratio1+ratio2))\n",
        "  x_train = x[:train_length]\n",
        "  y_train = y[:train_length]\n",
        "  x_valid = x[train_length:valid_length]\n",
        "  y_valid = y[train_length:valid_length]\n",
        "  x_test = x[valid_length:]\n",
        "  y_test = y[valid_length:]\n",
        "\n",
        "  return (x_train, y_train, x_valid,y_valid, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZNBME10eouE"
      },
      "outputs": [],
      "source": [
        "seen_x_train, seen_y_train,seen_x_valid, seen_y_valid,  seen_x_test, seen_y_test = data_split(seen_x,seen_y,0.4,0.4)\n",
        "\n",
        "deft_x_train, deft_y_train, deft_x_valid, deft_y_valid, deft_x_test, deft_y_test = data_split(kmean_z_train,kmean_y_train,0.4,0.4)\n",
        "print(deft_x_test.shape)\n",
        "print(deft_x_valid.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKqvb0st9vb5"
      },
      "outputs": [],
      "source": [
        "T = dict()\n",
        "intial_center = []\n",
        "for i in range(0,12):\n",
        "  T[i] = np.mean(data_dict[i][0:2000],axis = 0)\n",
        "  intial_center.append(T[i])\n",
        "intial_center = np.array(intial_center)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhWwiFMqR6uN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAbsof1lSur3"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "## initialize with a initial center\n",
        "Kmean = KMeans(n_clusters=12, init = intial_center )\n",
        "Kmean.fit(deft_x_valid)\n",
        "K_label_list = Kmean.labels_\n",
        "cluster_dict = dict()\n",
        "for i in range(len(K_label_list)):\n",
        "  cluster_dict[K_label_list[i]] = []\n",
        "\n",
        "for i in range(len(K_label_list)):\n",
        "  l = cluster_dict[K_label_list[i]]\n",
        "  l.append(deft_y_valid[i])\n",
        "  cluster_dict[K_label_list[i]] = l\n",
        "\n",
        "\n",
        "total_correct = 0\n",
        "total_data = 0\n",
        "total_unseen_correct = 0\n",
        "total_unseen_data = 0\n",
        "for i in range(12):\n",
        "  data_pair =dict()\n",
        "  unique, count = np.unique(cluster_dict[i], return_counts = True)\n",
        "  for j in range(len(unique)):\n",
        "    data_pair[unique[j]] = count[j]\n",
        "    # print(unique[j], count[j])\n",
        "  max_count = np.max(count)\n",
        "  rate = max_count / len(cluster_dict[i])\n",
        "  total_correct += max_count\n",
        "  total_data += len(cluster_dict[i])\n",
        "  max_idx = 0\n",
        "  for keys in data_pair:\n",
        "    if data_pair[keys] == max_count:\n",
        "      max_idx = keys\n",
        "  if max_idx == 10 or max_idx == 11:\n",
        "    total_unseen_correct += max_count\n",
        "    total_unseen_data += len(cluster_dict[i])\n",
        "\n",
        "  print('cluster: {}, label: {}, accuracy: {} '.format(i, max_idx, rate))\n",
        "  print(data_pair)\n",
        "  print(rate)\n",
        "\n",
        "final_acc = total_correct/total_data\n",
        "final_zsl_acc = total_unseen_correct/total_unseen_data\n",
        "print('gzsl acc : ', final_acc)\n",
        "print('zsl acc : ', final_zsl_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}