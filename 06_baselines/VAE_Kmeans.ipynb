{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout, Reshape\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Input\n",
        "from collections import Counter\n",
        "from numpy import savez_compressed\n",
        "from datetime import date\n",
        "from numpy import load\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout, Reshape, Lambda, Input, BatchNormalization, concatenate\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "attr_num = 3\n",
        "seen_num = 10\n",
        "unseen_num = 2"
      ],
      "metadata": {
        "id": "EHIeZHH4d2Zx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "VhgAckaOd-Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")"
      ],
      "metadata": {
        "id": "zC4W6JIueGTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_make_split(npz_file, train_percentage):\n",
        "    \"\"\"\n",
        "    Load training data (windows + one-hot labels) from compressed file. Split data into train and test set\n",
        "\n",
        "    Arguments:\n",
        "        - npz_file: The path to the *.npz file\n",
        "        - train_percentage: the percentage of data used for training (and not testing), e.g. 0.8\n",
        "    Returns:\n",
        "        A 4-tuple of train and test data with labels: (x_train, y_train, x_test, y_test)\n",
        "    \"\"\"\n",
        "    dict_data = load(npz_file)\n",
        "    x = dict_data['x']\n",
        "    y = dict_data['y']\n",
        "    train_length = int(len(x)*train_percentage)\n",
        "    x_train = x[:train_length]\n",
        "    y_train = y[:train_length]\n",
        "    x_test = x[train_length:]\n",
        "    y_test = y[train_length:]\n",
        "    return (x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "boYELme7eP69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(npz_file_seen)\n",
        "x_train, y_train, z_train, x_test, y_test, z_test = load_data_make_split(\"{}.npz\".format(npz_file_seen),0.8, attr_num )\n",
        "real_train_x, real_train_y, real_train_z, real_test_x, real_test_y, real_test_z  =  load_data_make_split(\"{}.npz\".format(npz_file_unseen), 0.8,attr_num )\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrBbrrMv_iTF",
        "outputId": "73c71d0d-3cd9-4e78-e442-c16b6036da26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/iot_device_classification/npz_windows/balanced_vae_kmeans_seen\n",
            "(96000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape the label to be onehot form\n",
        "labels_ohe= np.empty((y_train.shape[0],seen_num,1), dtype=np.float32)\n",
        "for i in range(y_train.shape[0]):\n",
        "  label_ohe = np.zeros((1,seen_num))\n",
        "  label_ohe[0][int(y_train[i])] = 1\n",
        "  labels_ohe[i] = label_ohe.T\n",
        "\n",
        "labels_ohe_test= np.empty((y_test.shape[0], seen_num, 1), dtype=np.float32)\n",
        "for i in range(y_test.shape[0]):\n",
        "  label_ohe = np.zeros((1,seen_num))\n",
        "  label_ohe[0][int(y_test[i])] = 1\n",
        "  labels_ohe_test[i] = label_ohe.T\n",
        "\n",
        "y_train = labels_ohe.reshape((y_train.shape[0],-1))\n",
        "y_test = labels_ohe_test.reshape((y_test.shape[0],-1))\n",
        "\n",
        "disable_eager_execution()\n",
        "\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 64\n",
        "test_size = x_test.shape[0]"
      ],
      "metadata": {
        "id": "tzc-WWeK_f1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = 64 # bacthsize\n",
        "n_x = 20 # feature dimension\n",
        "n_y = attr_num  # attribute vector\n",
        "n_z = 3  # noise dimension\n",
        "interNo = n_x  # number of neurons in the middle layer\n",
        "n_epoch = 10  # number of epoches\n",
        "\n",
        "# encoder input dimension\n",
        "input_ic = Input(shape=[n_x], name = 'img_class' )\n",
        "# attribute vector\n",
        "# cond  = Input(shape=[n_y] , name='class')\n",
        "temp_h_q = Dense(interNo, activation='relu')(input_ic)\n",
        "h_q_zd = Dropout(rate=0.2)(temp_h_q)\n",
        "h_q = Dense(interNo, activation='relu')(h_q_zd)\n",
        "# dense layer for mu\n",
        "temp_h_q_2 = Dense(n_z, activation='linear')(h_q)\n",
        "mu = Dense(n_z, activation='linear')(temp_h_q_2)\n",
        "# dense layer for log\n",
        "# log_sigma = Dense(n_z, activation='linear')(h_q)\n",
        "log_sigma = Dense(n_z, activation='linear')(temp_h_q_2)\n",
        "\n",
        "def sample_z(args):\n",
        "    mu, log_sigma = args\n",
        "    eps = tf.random.normal(shape=[n_z], mean=0., stddev=1.)\n",
        "    return mu + tf.exp(log_sigma / 2) * eps\n",
        "\n",
        "z = Lambda(sample_z)([mu, log_sigma])\n",
        "\n",
        "decoder_hidden = Dense(32, activation='relu')\n",
        "decoder_out = Dense(n_x, activation='linear')\n",
        "h_p = decoder_hidden(z)\n",
        "reconstr = decoder_out(h_p)\n",
        "vae = Model(inputs=[input_ic], outputs=[reconstr])\n",
        "\n",
        "encoder = Model(inputs=[input_ic], outputs=[z])\n",
        "\n",
        "d_in = Input(shape=[n_z])\n",
        "d_h = decoder_hidden(d_in)\n",
        "d_out = decoder_out(d_h)\n",
        "decoder = Model(d_in, d_out)\n",
        "\n",
        "def vae_loss(y_true, y_pred):\n",
        "    \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n",
        "    recon = tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true), axis=1)\n",
        "    # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
        "    kl = 0.5 * tf.keras.backend.sum(tf.exp(log_sigma) + tf.keras.backend.square(mu) - 1. - log_sigma, axis=1)\n",
        "    #print 'kl : ' + str(kl)\n",
        "    return recon + kl\n",
        "encoder.summary()\n",
        "decoder.summary()\n",
        "vae.compile(optimizer=\"adam\", loss=vae_loss)\n",
        "\n",
        "\n",
        "X_train = np.concatenate([x_train], axis=1)\n",
        "print(X_train.shape)\n",
        "print ('Fitting VAE Model...')\n",
        "vae.fit({'img_class' : X_train , 'class' : z_train}, x_train, batch_size=m, epochs=n_epoch)"
      ],
      "metadata": {
        "id": "jyaasNuRyTO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_reconstruction_loss(x_train, y_train, z_train, encoder, decoder, flag):\n",
        "\n",
        "  diff_set = dict()\n",
        "  sess = tf.compat.v1.Session()\n",
        "\n",
        "  for i in range(200):\n",
        "    a = x_train[i]\n",
        "    # flag = 0 seen flag = 1 unseen\n",
        "    if flag == 0:\n",
        "      b_1 = y_train[i]\n",
        "      for j in range(len(b_1)):\n",
        "        if b_1[j] == 1:\n",
        "          b = j\n",
        "    else:\n",
        "      b =  y_train[i]\n",
        "    c = z_train[i]\n",
        "    # print(a, b, c)\n",
        "    enc_ip = np.concatenate((a, c))\n",
        "    # print(enc_ip.shape)\n",
        "    latent = encoder.predict([[enc_ip],[c] ])\n",
        "    latent = latent.reshape((1,-1))\n",
        "    c = c.reshape((1,-1))\n",
        "\n",
        "    dec_ip = np.concatenate((latent, c) , axis=1)\n",
        "    pseudoTrainData_test = decoder.predict(dec_ip)\n",
        "    with sess.as_default():\n",
        "      diff =tf.keras.backend.mean(tf.keras.backend.square(pseudoTrainData_test- a), axis=1).eval()\n",
        "    if b not in diff_set.keys():\n",
        "      diff_set[b] = diff\n",
        "    else:\n",
        "      before = diff_set[b]\n",
        "      after = np.concatenate((before,diff))\n",
        "      diff_set[b] = after\n",
        "  for i in diff_set.keys():\n",
        "    print('mean of {} : {} '.format(i,np.mean(diff_set[i])))\n",
        "\n",
        "  return diff_set\n",
        "\n",
        "\n",
        "diff_set_seen = generate_reconstruction_loss(x_train, y_train, z_train, encoder, decoder, flag = 0)\n",
        "diff_set_unseen = generate_reconstruction_loss(real_train_x, real_train_y, real_train_z, encoder, decoder, flag = 1)"
      ],
      "metadata": {
        "id": "u1qt2xesAjk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, z_train, x_test, y_test, z_test = load_data_make_split(\"{}.npz\".format(npz_file_seen),0.8, attr_num )\n",
        "final_train_x = np.concatenate((real_train_x,x_test),axis = 0)\n",
        "print(final_train_x.shape)\n",
        "final_train_y = np.concatenate((real_train_y,y_test),axis = 0)\n",
        "print(final_train_y.shape)"
      ],
      "metadata": {
        "id": "RFVRMPK_e13s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_encoder_data(x_test, real_train_x, z_test, real_train_z,real_train_y,y_test ):\n",
        "  final_train_x = np.concatenate((real_train_x,x_test),axis = 0)\n",
        "  final_train_z = np.concatenate((real_train_z,z_test),axis = 0)\n",
        "  final_train_y = np.concatenate((real_train_y,y_test),axis = 0)\n",
        "  ec_ip = np.concatenate((final_train_x, final_train_z), axis=1)\n",
        "  # latent = encoder.predict([[enc_ip],[c] ])\n",
        "  embedding_Data = encoder.predict([ec_ip,final_train_z])\n",
        "  return embedding_Data, final_train_y"
      ],
      "metadata": {
        "id": "AEKdb7ZK4nCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmean_z_train, kmean_y_train = generate_encoder_data(x_test, real_train_x, z_test, real_train_z,real_train_y,y_test)"
      ],
      "metadata": {
        "id": "NRDtPj-he_o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "lCISDxrgfCr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kmean = KMeans(n_clusters=12)\n",
        "Kmean.fit(kmean_z_train)"
      ],
      "metadata": {
        "id": "p4YLXh2LfDmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K_label_list = Kmean.labels_\n",
        "cluster_dict = dict()\n",
        "for i in range(len(K_label_list)):\n",
        "  cluster_dict[K_label_list[i]] = []"
      ],
      "metadata": {
        "id": "aF8WbUYwrW3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(K_label_list)):\n",
        "  l = cluster_dict[K_label_list[i]]\n",
        "  l.append(kmean_y_train[i])\n",
        "  cluster_dict[K_label_list[i]] = l"
      ],
      "metadata": {
        "id": "Uw1oBPQ3rZEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_correct = 0\n",
        "total_data = 0\n",
        "total_unseen_correct = 0\n",
        "total_unseen_data = 0\n",
        "for i in range(12):\n",
        "  data_pair =dict()\n",
        "  unique, count = np.unique(cluster_dict[i], return_counts = True)\n",
        "  for j in range(len(unique)):\n",
        "    data_pair[unique[j]] = count[j]\n",
        "    # print(unique[j], count[j])\n",
        "  max_count = np.max(count)\n",
        "  rate = max_count / len(cluster_dict[i])\n",
        "  total_correct += max_count\n",
        "  total_data += len(cluster_dict[i])\n",
        "  max_idx = 0\n",
        "  for keys in data_pair:\n",
        "    if data_pair[keys] == max_count:\n",
        "      max_idx = keys\n",
        "  if max_idx == 10 or max_idx == 11:\n",
        "    total_unseen_correct += max_count\n",
        "    total_unseen_data += len(cluster_dict[i])\n",
        "\n",
        "  print('cluster: {}, label: {}, accuracy: {} '.format(i, max_idx, rate))\n",
        "  print(data_pair)\n",
        "\n",
        "final_acc = total_correct/total_data\n",
        "final_zsl_acc = total_unseen_correct/total_unseen_data\n",
        "print('gzsl acc : ', final_acc)\n",
        "print('zsl acc : ', final_zsl_acc)"
      ],
      "metadata": {
        "id": "HUA0WSWerdWb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}